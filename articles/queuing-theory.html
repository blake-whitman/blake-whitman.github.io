<!DOCTYPE html>
<html>
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-108813396-1"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-108813396-1');
    </script>

    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=Edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="../static/css/style.css">
    <link href="../static/css/code.css" rel="stylesheet" type="text/css">
    <title>Empirical analysis of queuing theory on transit data - Blake Whitman</title>
  </head>
  <body>
<span class="breadcrumb"><a href="../index.html">Home</a> &gt; <a href="https://blake-whitman.github.io/#articles">Articles</a> &gt; Empirical analysis of queuing theory on transit data</span>
<h1>Empirical analysis of queuing theory on transit data</h1>
<p><strong>Abstract</strong> The problem of analyzing transit data using queuing theory has been a topic left mostly unexplored. Recently, many useful results have been obtained related to transportation data and reduced waiting times. Problems currently under investigation include the Waiting Time Paradox [5] and applications of real-time transit data to wait times [4].</p>
<p>Based on the work of Liu and Miller (Transportation Research Part A, p. 167–179, 2020), we introduce and study a transportation simulation involving approximated data sets. Through an interpretation of the notion of queuing theory, we verify whether this simulation fulfills the properties of a Poisson Distribu- tion, and if not, which distributions most accurately describe a queuing theory process with multiple queues.</p>
<h2>Introduction</h2>
<p>The advent and widespread use of transportation data is revolutionizing the manner in which passengers approach traveling in their daily lives. Technology is now in place to offer each traveler autonomy in selecting transportation that best fits their schedule and arrival time preferences. Several apps in their infancy, such as Citymapper, Transit, and Moovit, are able to automatically render their predicted arrival times based on local traffic obstructions. Moreover, many local transit engineering agencies are publicly releasing the data with which these applications run, enabling further studies on the accumulated data.</p>
<p>One such area of exploration is in the realm of waiting times. Our study consists of analyzing the distribution of waiting times among a revolving system of buses and bus stops. In this context, each waiting time is measured from the time a consumer arrives at a bus stop, up until the subsequent arrival of a bus at that specific stop with availability for the passenger. We then perform analysis to view whether the empirical distribution closely approximates a Poisson Process, which can be thought of as a model for a series of events in which the average time between events is known, but the exact timing of events remains random [3]. For our purposes, we are aware of the computed average waiting time, while at the same time not knowing the exact dispersal of said times. There is a degree of randomness involved, which we hypothesize maintains the integrity of the Poisson Process. After engaging in a series of tests, the final distribution will be evident and utilized for comparison with regard to the expected outcome.</p>
<p>In the next section, we will reveal more of the factors and variables involved in the experiment that ensure randomness, while also taking a look at the theoretical outcome as well. In Section III, we analyze the results collected in Section II, while providing more detailed data visualization strategies to become more cognizant of the process. We conclude this study with a look at the findings we have uncovered, their respective significance, and further avenues in which modifications still exist.</p>
<h2>Methodology</h2>
<p>In this section, we discuss the various parameters and data structures used throughout the simulation. Given a multitude of constants and independent variables, we seek to randomly allocate passengers to buses. We then discuss the main body of the simulation program, before delving into the specific circumstances within it. The theoretical calculations are described in detail at the end of this section. These results will be compared to the empirical rendition in Section III.</p>
<p>The testing location and premise is largely based on the Central Ohio Transit Authority (COTA) bus system in Columbus, Ohio. Data and bus stop names are merely estimated, though, and are meant only as an approximation to real wait times in the city. Columbus is a densely populated urban area, allowing for these results to be reproduced in other major city systems as well. Therefore, the overarching ideas utilized in this paper are applicable to other cities and transportation methods, so long as proper data source alterations are made.</p>
<ol>
<li>In order to understand the different values used in this experiment, it may help to first be given a general overview of the setup. To begin with, there are a predetermined allotment of buses and bus stops, and the buses are tasked with revolving around the bus stops. Starting with the simplest case, let us assume there is 1 bus and n bus stops. Then, the bus will first visit bus stop 1, then bus stop 2, and continue onward until it reaches the nth bus stop. In other words, bus i will travel in order from stop 1, 2, . . . j, j ∈ Z. After that point, it will begin again at bus stop 1 and proceed with this pattern until the simulation resolves. This idea remains consistent independent of the number of buses in use. The only modification is that the buses will be evenly distributed to maximize their route efficiency. Maintaining an equivalent distance between buses is addressed in the "Variables" section below.</li>
<li>A myriad of constants were utilized amid the simulation process. These values were often approxi- mated in order to simplify the testing process, and each can be altered as needed based on the particular bus system under surveillance. During testing, the following equivalencies were used:
<blockquote>
<p>number of buses (numBuses) = 4</p>
<p>number of bus stops (numStops) = 5</p>
<p>bus capacity (capacity) = 40</p>
</blockquote>
The simplifying assumption was made that each bus stop is an equidistant length from both the preceding and subsequent bus stop. To account for this, the metric bus stop interval can be defined as the time it takes for a bus to go from stop i -> stop i+1. For these trials, that length of time is 20 minutes. Thus, bus 1 will first reach bus stop 1 in 20 minutes, and it will again reach bus stop 1 at a time of 120 minutes in our simulation. Or, for an all encompassing solution, consider the following theorem:
<blockquote>
<p><strong>Theorem 2.1:</strong> Bus i will reach stop j in n minutes, and bus i returns to stop j again in [(numStops * bus stop interval) + n] minutes.</p>
</blockquote>
</li>
<li>
Not all values used were constants, so now we will provide insight into the various components of the program that were repeatedly updated using some form of randomness. In an effort to compare trials of multiple lengths, duration of simulation was varied. The original experiment length was set to 1,440 minutes (1 day), but longer trials were incorporated as well. We hypothesize that with more runtime, we will encounter a closer approximation to a Poisson Process. By altering the trial lengths, this idea will be closely monitored. Another value that fluctuates throughout the trials is the number of passengers departing each bus. The number departing depends entirely on which stop the bus encounters, as well as the total number of passengers on the bus upon arrival. The relative usage of the bus stop in the system will lead to variant departure rates. For instance, the Recreation and Physical Activity Center (RPAC) at Ohio State is a well visited destination, and as such, warrants a higher rate of disembarking passengers than St. John Arena. This thought is taken into account when determining passenger exchanges at each bus stop. As previously mentioned in the aˆOverview,aˆ the distance between buses must remain the same for all buses involved. Thus, a new term, bus distance interval, is defined as the allotted space between two buses. It can be calculated with the following theorem:
<blockquote>
<p><strong>Theorem 2.2:</strong> bus distance interval = (bus stop interval * numStops) / numBuses</p>
</blockquote>
In our experiment, this value was computed to be 25 minutes, which means each bus is initially sent off 25 minutes after the one in front of it until all buses are running. The gap between buses will then perpetually remain 25 minutes until the end of the trials. With a greater or lesser number of buses or stops, this metric will change. Its aim is to distribute bus inventory evenly throughout the entirety of the system. To properly address the randomness aspect of a Poisson Process, we had to ensure that consumers arrived at bus stops randomly throughout the simulation. To achieve this, each bus stop was assigned a random digit between [arriveLow, arriveHigh]. Both arriveLow and arriveHigh are integer values, in minutes, that the random digit falls between, inclusively. These numbers were again based on the usage rate of each bus stop in consideration, with more densely populated bus stops having lower arriveLow and arriveHigh numbers. Lower values indicate a passenger arriving in fewer minutes, on average, than corresponding higher values. Expected Rate of Arrival (eROA) represents the mean value of arriveLow and arriveHigh. As eROA increases, the flow of arriving passengers will decrease. Here are the figures used for our five bus stops, to provide a clearer insight into the range considered for each arriving passenger: As mentioned previously, a lower expected ROA points toward a highly traversed bus stop, so we can conclude that the RPAC was pinpointed as the single most visited stop out of the five routes. Conversely, Knowlton Hall and the Student Union experience more infrequent customer arrivals. As we move forward, we can compare the wait times of these two unique situations to see the impact, if any, on their respective wait time distributions.
</li>
</ol>
    
    
    
    
<p>Good scientific measurements come with uncertainty. A ruler or measuring tape is
only so precise. When it comes to machine learning, though, this focus on
uncertainty disappears.</p>
<p>For example, see this common formulation of the learning problem from the book
<em>Learning from Data</em> <a href="#fn0">[0]</a> (no shade -- I love this book):</p>
<blockquote>
<p>There is a target to be learned. It is unknown to us. We have examples
generated by the target. The learning algorithm uses these examples to look
for a hypothesis that approximates the target.</p>
</blockquote>
<p>This unknown target function they call <em>f</em>: <em>X</em> -&gt; <em>Y</em> (where <em>X</em> is the input/feature
space and <em>Y</em> is the output/target space). The hypothesis approximating the
target they denote as <em>g</em>: <em>X</em> -&gt; <em>Y</em> (and, if our learning algorithm is successful,
we can say <em>g ≈ f</em>).</p>
<p>In the case of regression, the output of our learning algorithm is a function
which produces a continuous-valued output. But this output is a point estimate.
It has no sense of uncertainty <a href="#fn1">[1]</a>!</p>
<p>Others have of course noted the importance of uncertainty estimation before me.
One such person is José Hernández-Orallo (a professor at Polytechnic University
of Valencia), whose paper <a rel="nofollow" href="https://dl.acm.org/doi/abs/10.1145/2641758"><em>Probabilistic reframing for cost-sensitive
regression</em></a> I found while
researching <a href="/articles/how-to-handle-class-imbalance.html">class
imbalance</a>. I would
be misrepresenting his work to claim this paper is solely about
uncertainty/reliability estimation, but he describes some neat ideas
worth exploring.</p>
<p>Rather than finding a function <em>g</em>: <em>X</em> -&gt; <em>Y</em> approximating the true underlying
function <em>f</em>, we could instead seek to find a probability density function <em>h(y |
x)</em>. In other words, a function to which we can still pass some features (<em>x</em>) but
one describing a distribution instead of a point estimate. Because we are
estimating a probability density function conditioned on the input features,
this idea is called conditional density estimation.</p>
<p>To hear Hernández-Orallo tell it, many methods for conditional density
estimation are suboptimal. The mean of the distributions they output is
typically worse than a point estimate would have been. They are often slow. And
in many cases, the distributions don't end up being multi-modal anyway. Thus,
the paper asserts we can get by with a method to provide a
normal (Gaussian) density function for most cases.</p>
<p>Normal distributions are parametrized by a mean and a standard deviation. Taking
a point estimate (from any regression model) as the mean, we still need to
determine the standard deviation. The paper describes a few different approaches
for doing this (and is worth reading if you have the time). For the sake of this
post, I'll focus on a technique called "univariate k-nearest comparison". A
simple Python implementation follows:</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">univariate_knearest_comparison</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">test_point</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">all_preds</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
    <span class="n">Q</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="n">all_preds</span><span class="p">,</span> <span class="n">dataset</span><span class="p">[</span><span class="s2">&quot;Weight&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
    <span class="n">prediction</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test_point</span><span class="p">)</span>
    <span class="n">neighbors</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span>
        <span class="p">((</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_true</span><span class="p">)</span> <span class="k">for</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">y_true</span> <span class="ow">in</span> <span class="n">Q</span><span class="p">),</span>
        <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">t</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">prediction</span><span class="p">),</span>
    <span class="p">)[:</span><span class="n">k</span><span class="p">]</span>
    <span class="n">variance_estimate</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">k</span><span class="p">)</span> <span class="o">*</span> <span class="nb">sum</span><span class="p">(</span>
        <span class="p">(</span><span class="n">prediction</span> <span class="o">-</span> <span class="n">y_true</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">y_true</span> <span class="ow">in</span> <span class="n">neighbors</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">prediction</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">variance_estimate</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>

<p>Hernández-Orallo describes this procedure as looking "for the closest
estimations in the training set to the estimation for example <em>x</em>", then comparing
"their true values with the estimation for <em>x</em>".</p>
<p>This technique is cool because it can be applied to any regression model. By
using the training set (or a validation set) in this clever way, we can enrich
any model with the ability to estimate uncertainty (thus gaining the second
aspect of what we've <a href="/articles/trustworthy-models.html">been calling trustworthy
models</a>).</p>
<p>If you've not read previous posts in this series, we've been working with a
dataset about fish. From their dimensions, we're trying to predict their weight.
It's totally a toy/unrealistic problem, but it's pedagogically useful.</p>
<p>We'll start by training a linear model.</p>
<div class="highlight"><pre><span></span><span class="n">ct</span> <span class="o">=</span> <span class="n">ColumnTransformer</span><span class="p">([</span>
    <span class="p">(</span><span class="s1">&#39;scale&#39;</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">(),</span> <span class="p">[</span><span class="s1">&#39;Length1&#39;</span><span class="p">,</span> <span class="s1">&#39;Length2&#39;</span><span class="p">,</span> <span class="s1">&#39;Length3&#39;</span><span class="p">,</span> <span class="s1">&#39;Height&#39;</span><span class="p">,</span> <span class="s1">&#39;Width&#39;</span><span class="p">]),</span>
    <span class="p">(</span><span class="s1">&#39;ohe&#39;</span><span class="p">,</span> <span class="n">OneHotEncoder</span><span class="p">(),</span> <span class="p">[</span><span class="s1">&#39;Species&#39;</span><span class="p">]),</span>
<span class="p">])</span>

<span class="n">pipe</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">ct</span><span class="p">,</span> <span class="n">lm</span><span class="o">.</span><span class="n">LinearRegression</span><span class="p">())</span>
<span class="n">pipe</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">fish</span><span class="p">,</span> <span class="n">fish</span><span class="p">[</span><span class="s1">&#39;Weight&#39;</span><span class="p">])</span>
</pre></div>

<p>Then, we'll run the univariate k-nearest comparison function from above.</p>
<div class="highlight"><pre><span></span><span class="n">new_fish</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="p">{</span>
            <span class="s2">&quot;Species&quot;</span><span class="p">:</span> <span class="s2">&quot;Bream&quot;</span><span class="p">,</span>
            <span class="s2">&quot;Weight&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
            <span class="s2">&quot;Length1&quot;</span><span class="p">:</span> <span class="mf">31.3</span><span class="p">,</span>
            <span class="s2">&quot;Length2&quot;</span><span class="p">:</span> <span class="mi">34</span><span class="p">,</span>
            <span class="s2">&quot;Length3&quot;</span><span class="p">:</span> <span class="mf">39.5</span><span class="p">,</span>
            <span class="s2">&quot;Height&quot;</span><span class="p">:</span> <span class="mf">15.1285</span><span class="p">,</span>
            <span class="s2">&quot;Width&quot;</span><span class="p">:</span> <span class="mf">5.5695</span><span class="p">,</span>
        <span class="p">}</span>
    <span class="p">]</span>
<span class="p">)</span>

<span class="n">pred</span><span class="p">,</span> <span class="n">var</span> <span class="o">=</span> <span class="n">univariate_knearest_comparison</span><span class="p">(</span><span class="n">pipe</span><span class="p">,</span> <span class="n">fish</span><span class="p">,</span> <span class="n">new_fish</span><span class="p">)</span>
<span class="c1"># (646.1153309725989, 4896.726887004621)</span>
</pre></div>

<p>With our prediction and variance estimate in hand, we can draw a normal
distribution.</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scipy.stats</span> <span class="kn">as</span> <span class="nn">st</span>

<span class="k">def</span> <span class="nf">plot_normal</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">var</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;coral&#39;</span><span class="p">,</span> <span class="n">do_lims</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
    <span class="n">width</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var</span><span class="p">)</span> <span class="o">*</span> <span class="mf">3.8</span>
    <span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span> <span class="o">=</span> <span class="n">pred</span> <span class="o">-</span> <span class="n">width</span><span class="p">,</span> <span class="n">pred</span> <span class="o">+</span> <span class="n">width</span>

    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>

    <span class="n">y</span> <span class="o">=</span> <span class="n">st</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var</span><span class="p">))</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">do_lims</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span><span class="p">)</span>

        <span class="n">ylo</span><span class="p">,</span> <span class="n">yhi</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">()</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">yhi</span><span class="p">)</span>

        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Conditional density of fish weight given features&#39;</span><span class="p">)</span>

<span class="n">plot_normal</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">var</span><span class="p">)</span>
</pre></div>

<p><img src="/static/img/uknc0.png"></p>
<p>Here's a little graph with conditional density estimates for several different
fish on it.</p>
<p><img src="/static/img/uknc1.png"></p>
<p>This is a strong step in the direction of being more scientific in our modeling
efforts. We've examined a few methods for uncertainty estimation in this series,
and we'll evaluate the quality of these techniques at a later date.</p>
<p><em>If you found this interesting, consider <a rel="nofollow" href="https://twitter.com/SamuelDataT">following me on Twitter</a>.</em></p>
<hr />
<h3>Footnotes</h3>
<ol start="0">
<li id="fn0">Abu-Mostafa, Y. S., Magdon-Ismail, M., &amp; Lin, H. (2012).
_Learning from data: A short course_. United States: AMLBook.com.</li>
<li id="fn1">I recognize that I'm equivocating on the word "uncertainty" to some extent.
Still, I think this is a useful idea even if only as an analogy.</li>
</ol>
  </body>
</html>
